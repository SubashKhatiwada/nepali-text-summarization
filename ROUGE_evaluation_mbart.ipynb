{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27059833-c8fa-4e55-ba7c-f244b766fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████                                | 23/89 [03:20<09:55,  9.02s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"./mbart-nepali-summarization\").to(device)\n",
    "\n",
    "def batch_summarize(texts, batch_size=4, max_length=150, min_length=50):\n",
    "    \"\"\"Batch summarize Nepali texts using mBART\"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    # Set Nepali language code once\n",
    "    tokenizer.src_lang = \"ne_NP\"\n",
    "    forced_bos_token_id = tokenizer.lang_code_to_id[\"ne_NP\"]\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize batch with proper padding\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate summaries\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            forced_bos_token_id=forced_bos_token_id\n",
    "        )\n",
    "        \n",
    "        # Decode batch\n",
    "        batch_summaries = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        summaries.extend(batch_summaries)\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# Load test data (adjust column name as needed)\n",
    "test_df = pd.read_csv(\"test_dataset.csv\")\n",
    "texts = test_df[\"text\"].tolist()  # Assuming column named \"text\"\n",
    "\n",
    "# Process in batches\n",
    "print(\"Generating summaries...\")\n",
    "test_df[\"predicted_summary\"] = batch_summarize(texts)\n",
    "\n",
    "# Save results\n",
    "output_path = \"mbart_predictions_with_summaries.csv\"\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nPredictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d495dc86-574c-450f-9056-302fe8b2d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "ROUGE-1: f=0.1893, p=0.1700, r=0.2266\n",
      "ROUGE-2: f=0.0642, p=0.0558, r=0.0806\n",
      "ROUGE-L: f=0.1685, p=0.1509, r=0.2021\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "\n",
    "# Load your predictions file\n",
    "predictions_df = pd.read_csv(\"mbart_predictions_with_summaries.csv\")\n",
    "\n",
    "# Initialize ROUGE evaluator\n",
    "rouge = Rouge()\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = rouge.get_scores(\n",
    "    predictions_df[\"predicted_summary\"].tolist(),  # Generated summaries\n",
    "    predictions_df[\"summary\"].tolist(),           # Reference summaries (gold standard)\n",
    "    avg=True\n",
    ")\n",
    "\n",
    "# Print formatted results\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: f={scores['rouge-1']['f']:.4f}, p={scores['rouge-1']['p']:.4f}, r={scores['rouge-1']['r']:.4f}\")\n",
    "print(f\"ROUGE-2: f={scores['rouge-2']['f']:.4f}, p={scores['rouge-2']['p']:.4f}, r={scores['rouge-2']['r']:.4f}\")\n",
    "print(f\"ROUGE-L: f={scores['rouge-l']['f']:.4f}, p={scores['rouge-l']['p']:.4f}, r={scores['rouge-l']['r']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7f53f-3a73-4ad1-b1d1-8820b5436aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
